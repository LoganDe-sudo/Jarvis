
# ğŸ§  Jarvis â€“ Dynamic Vocabulary Transformer

**Jarvis** is a GPT-style transformer language model that **supports dynamic vocabulary expansion** â€” enabling the model to learn new words/tokens *on the fly* without restarting training or breaking core model behavior. It's ideal for **AI assistants that grow smarter over time** with custom data.

---

## ğŸš€ Features

- âœ… **Dynamic Embedding + Head**: `ExpandableEmbedding` & `ExpandableLinearHead` grow without reinitializing
- âœ… **Safe vocabulary expansion**: new tokens can be added without resetting training
- âš ï¸ **Flexible model loading**: existing weights are reused, and only mismatched vocab layers are skipped
- âœ… **No training loss on expansion**: continue training even after tokenizer grows
- âœ… Integrated with HuggingFace-compatible tokenizer (`tokenizers` lib)
- âœ… Supports gradient clipping, AMP training, early stopping
- ğŸ“‰ Tracks loss, perplexity during training

---

## ğŸ—ï¸ Architecture

- Transformer encoder stack (custom-built from scratch)
- `MultiHeadAttention` using PyTorch's native implementation
- Sinusoidal positional encoding
- Causal (auto-regressive) masking
- Modular layer design for easy scaling (layers, dimensions, vocab)

---

## ğŸ“¦ Project Structure

```
jarvis/
â”œâ”€â”€ model.py             # Core Transformer + dynamic vocab model
â”œâ”€â”€ tokenizer_setup.py   # BPE Tokenizer training and updates
â”œâ”€â”€ train.py             # Continual training pipeline
â”œâ”€â”€ generate.py          # Inference script for text generation
â”œâ”€â”€ testchat.py          # Interactive testing script
```

---

## ğŸ Quick Start

1. **Install dependencies**
```bash
pip install torch tokenizers scikit-learn tqdm
```

2. **Prepare your data**  
In `.jsonl` format:
```json
{"input": "What's your name?", "response": "I'm Jarvis."}
```

3. **Train the model**
```bash
python train.py
```

4. **Generate output**
```bash
python generate.py
```

---

## ğŸ” Continual Learning Flow

This is how Jarvis evolves:

```
[train on dataset A] â†’ [add new data] â†’ [tokenizer grows] â†’ [model.expand_vocab()] â†’ [resume training] â†’ [repeat forever]
```

Even with new tokens, Jarvis doesnâ€™t lose old knowledge.

---

## ğŸ§ª What Makes This Special?

Normally, changing the tokenizer breaks the model:

- âŒ You'd need to **start training from scratch** or
- âŒ Youâ€™d get **state_dict loading errors** due to mismatched sizes

**But Jarvis fixes this:**

- âœ¨ `ExpandableEmbedding`: expands to fit new tokens, keeps old weights
- âœ¨ `ExpandableLinearHead`: maps expanded embeddings to new vocab
- âœ¨ `load_flexible_state_dict()`: loads all compatible weights, safely skips mismatches

ğŸ§  *"Partial mismatch? No problem â€” the brain remains intact."*

---

## âœ… Example Result

- You train on Set B
- Then Set A
- Then again Set B  
â†’ Perplexity **starts low again** â€” proving Jarvis **remembers past datasets**, even with vocab growth.

---

## âš ï¸ Note on Model Loading

Jarvis **does flexible (not 100% identical) loading**:
- It reuses *all matching parameters*
- It **skips** old embedding/head weights if vocab size changed
- But this is *intentional* to preserve training

---

## ğŸ­ Scale & Future Vision

Jarvis is **modular** â€” you can:

- ğŸ”§ Increase model depth, attention heads, vocab size
- ğŸ§  Train it on larger datasets
- ğŸ”Œ Plug it into tools (voice input, web search, APIs)

Big companies with large compute can adopt this **as a lifelong AI learner framework**.

---

## ğŸ“£ Credits

Crafted with care by **Logesh (a.k.a. Jarvis Creator)**  
> "Let your model grow smarter like a real brain â€” forever." ğŸ’¡
